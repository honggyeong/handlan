# HANDLAN: 실시간 수어(손동작) 인식 시스템

## 프로젝트 소개

**HANDLAN**은 청각장애인과 비장애인 간의 소통을 돕기 위해, 컴퓨터 비전과 인공지능 기술을 활용하여 손동작(수어, 즉 손으로 하는 언어)을 실시간으로 인식하는 시스템입니다.

이 프로젝트는 동아리 주제 탐구 활동의 일환으로 진행되었으며, 학교 선생님과 학생 누구나 쉽게 이해하고 활용할 수 있도록 구성하였습니다.

---

## 프로젝트 동기 및 수학적 배경

- 작년 수학2 교과에서 Relu 함수와 Sigmoid 함수와 같은 활성화 함수에서의 역전파 계산 방법을 학습하며, **LSTM의 활성 함수로 사용되는 다양한 함수에 대해 심화적으로 탐구하고자 하는 의지**가 이번 프로젝트의 직접적인 동기가 되었습니다. LSTM을 사용하여 딥러닝을 시켜 연속적인 움직임인 수어를 분석해보아야겠다고 생각하게 되었습니다.

---

## 프로젝트 목적
- **수어(손동작) 인식**을 통해 청각장애인의 의사소통을 지원
- 인공지능, 딥러닝, 컴퓨터 비전 등 최신 IT 기술을 직접 체험
- 영상 데이터 수집, 모델 학습, 실시간 예측 등 전체 인공지능 파이프라인 경험

---

## 기대 효과
- 수어를 모르는 사람도 손동작만으로 컴퓨터와 소통 가능
- 인공지능 모델의 원리와 실제 적용 과정을 쉽게 체험
- 동아리/학교 내 인공지능 교육 및 체험 활동 자료로 활용

---

## 프로젝트에 사용된 전문적 지식 및 핵심 기술

### 1. 컴퓨터 비전(Computer Vision)
- **MediaPipe**: 구글에서 개발한 실시간 손 관절(랜드마크) 추출 라이브러리. 손의 21개 주요 관절 위치를 영상에서 자동으로 추출.
- **OpenCV**: 영상(웹캠/동영상) 처리, 프레임 캡처, 실시간 화면 표시 등 영상 데이터 입출력 전반 담당.

### 2. 딥러닝(Deep Learning)
- **LSTM(Long Short-Term Memory)**: 시계열(시간에 따라 변하는) 데이터에 특화된 순환 신경망(RNN) 구조. 손동작처럼 연속적인 움직임을 인식하는 데 적합.
- **TensorFlow/Keras**: 딥러닝 모델 설계, 학습, 예측 등 전체 인공지능 파이프라인 구현.

### 3. 데이터 전처리 및 자동화
- **Numpy, Pandas**: 대용량 데이터(랜드마크 시퀀스, 라벨 등) 처리 및 저장.
- **yt-dlp**: 유튜브 등에서 수어 영상을 자동으로 다운로드하여 데이터셋 구축.

---

## 초기 설계(구현 전 생각했던 구조 다이어그램)

```mermaid
flowchart TD
    A[웹캠/영상 입력] --> B[손 랜드마크 추출(MediaPipe)]
    B --> C[시퀀스 데이터 생성(3초 단위)]
    C --> D[LSTM 모델 학습]
    D --> E[실시간 예측]
    E --> F[예측 결과 화면 표시]
```

- **A**: 웹캠/동영상 등에서 실시간 영상 입력
- **B**: MediaPipe로 손 관절(랜드마크) 추출
- **C**: 3초(90프레임) 단위로 시퀀스 데이터 생성
- **D**: LSTM 딥러닝 모델로 학습
- **E**: 실시간 손동작 예측
- **F**: 예측 결과를 화면에 표시

---

## 실제 개발 활동 과정(타임라인)

1. **문제 정의 및 목표 설정**
   - 청각장애인과 비장애인 소통을 돕는 실시간 수어 인식 시스템 목표 수립
2. **핵심 기술 조사**
   - MediaPipe, LSTM, OpenCV 등 관련 기술 조사 및 샘플 코드 실습
3. **데이터 수집 자동화**
   - 유튜브/직접 촬영 영상에서 손 랜드마크 자동 추출 코드 구현
   - 라벨링 및 시퀀스 데이터 자동 저장
4. **딥러닝 모델 설계 및 학습**
   - LSTM 기반 분류기 설계, 시퀀스 데이터로 학습
   - 학습 데이터 부족 시 직접 녹화로 데이터 보강
5. **실시간 예측 및 UX 개선**
   - 스페이스바로 녹화/예측, 예측 결과 2초간 표시 등 사용자 경험 개선
6. **문제점 발견 및 반복 개선**
   - 예측 정확도, UX, 데이터 품질 등 문제점 발견 시 코드/데이터/모델 반복 개선
7. **최종 정리 및 문서화**
   - 전체 파이프라인 자동화, README/설명서 작성, 깃허브 업로드

---

## 개발 중 발생했던 문제점과 해결 과정

### 1. **데이터 부족/불균형 문제**
- **문제**: 수어별 데이터가 적거나 한쪽 라벨만 많으면 예측이 한쪽으로 쏠림
- **해결**: mp4/유튜브 영상 자동 다운로드, 직접 녹화 등 다양한 방식으로 데이터 보강

### 2. **시퀀스 길이 불일치**
- **문제**: 학습/예측 시 시퀀스 길이(프레임 수)가 다르면 성능 저하
- **해결**: 모든 파이프라인에서 3초(90프레임)로 통일, 코드 내 변수 일괄 수정

### 3. **실시간 예측 UX 문제**
- **문제**: 스페이스바 이벤트 처리, 예측 결과 표시 타이밍 등에서 버그 발생
- **해결**: 이벤트 처리 로직 개선, 예측 결과 2초간 유지 등 UX 반복 개선

### 4. **카메라/환경 문제**
- **문제**: macOS에서 OpenCV 창이 안 뜨거나, 카메라 권한 문제 발생
- **해결**: 시스템 환경설정에서 Python/터미널에 화면 녹화/카메라 권한 부여, 카메라 연결 체크 코드 추가

### 5. **모델 성능 한계**
- **문제**: 데이터가 적거나 동작이 비슷할 때 예측 정확도 한계
- **해결**: 데이터 다양성 확보, LSTM 구조/epoch 조정, 직접 녹화 데이터 추가 등으로 성능 개선

---

## 프로젝트 회고 및 느낀 점
- 인공지능, 컴퓨터 비전, 데이터 자동화 등 다양한 최신 기술을 직접 체험할 수 있었음
- 실제로 손동작을 녹화하고, 인공지능이 예측하는 과정을 보며 기술의 원리를 몸소 이해할 수 있었음
- 문제 발생 시 원인을 분석하고, 반복적으로 개선하는 과정에서 문제해결력과 협업 능력이 크게 향상됨
- 동아리/학교에서 인공지능 교육 및 체험 활동 자료로 활용할 수 있어 뿌듯함

---

## 동작 흐름(한눈에 보기)

1. **손동작 데이터 수집**
   - 유튜브/직접 촬영한 영상을 이용해 손의 움직임(랜드마크) 데이터를 수집합니다.
2. **데이터 전처리 및 학습**
   - 손동작 데이터를 인공지능 모델(LSTM)에 학습시킵니다.
3. **실시간 예측**
   - 웹캠으로 손동작을 3초간 녹화하면, 컴퓨터가 어떤 수어(hello, thankyou, goodbye 등)인지 실시간으로 예측해줍니다.

---

## 주요 기술 및 도구
- **MediaPipe**: 손의 21개 주요 관절 위치(랜드마크) 추출
- **TensorFlow/Keras**: LSTM(순환 신경망) 기반 딥러닝 모델 학습
- **OpenCV**: 웹캠 영상 처리 및 실시간 화면 표시
- **Python**: 전체 파이프라인 구현

---

## 주요 파일 설명

- `extract_sequence_from_videos.py` : 영상에서 손동작(3초) 시퀀스 자동 추출
- `train_lstm.py` : 손동작 시퀀스 데이터로 LSTM 모델 학습
- `realtime_lstm_inference.py` : 웹캠으로 실시간 손동작 인식 및 예측
- `record_sequence.py`, `record_sequence_labeled.py` : 직접 손동작 녹화 및 데이터 저장
- `youtube_crawl_and_label.py` : 유튜브에서 수어 영상 자동 다운로드
- `handlan/` : 파이프라인 전체 코드 및 유틸리티 모음

---

## 실행 환경 및 준비 방법
- Python 3.8 이상
- 필수 패키지: `opencv-python`, `mediapipe`, `tensorflow`, `scikit-learn`, `numpy`, `pandas`, `yt-dlp` 등
- 설치 예시:
  ```bash
  pip install opencv-python mediapipe tensorflow scikit-learn numpy pandas yt-dlp
  ```

---

## 사용법(예시)

1. **영상에서 손동작 데이터 추출**
   ```bash
   python3 extract_sequence_from_videos.py
   ```
2. **LSTM 모델 학습**
   ```bash
   python3 handlan/train_lstm.py
   ```
3. **실시간 예측 실행**
   ```bash
   python3 realtime_lstm_inference.py
   ```

---

## 활용 예시
- 동아리 시간에 직접 손동작을 녹화해보고, 인공지능이 어떤 수어인지 맞추는 체험
- 다양한 수어(hello, thankyou, goodbye 등)를 추가해보고, 모델의 성능 변화를 실험
- 영상 데이터(유튜브 등)로 데이터셋을 확장하여 더 많은 수어 인식 도전

---

## 참고자료 및 학습 링크
- [MediaPipe 공식 문서](https://google.github.io/mediapipe/solutions/hands.html)
- [TensorFlow 공식 문서](https://www.tensorflow.org/)
- [OpenCV 공식 문서](https://opencv.org/)
- [유튜브 수어 데이터셋 예시: WLASL](https://github.com/dxli94/WLASL)

---

## 문의 및 기여
- 프로젝트 문의: honggyeong (github.com/honggyeong)
- 누구나 자유롭게 코드/데이터를 수정·확장하여 활용할 수 있습니다.

---

> **이 프로젝트는 동아리 주제 탐구 및 인공지능 체험 교육을 위해 제작되었습니다.**
> 
> 학교 선생님, 학생 모두가 쉽게 따라할 수 있도록 최대한 친절하게 설명하였으니, 궁금한 점은 언제든 문의해 주세요! 
